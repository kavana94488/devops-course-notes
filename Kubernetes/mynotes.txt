node is an object of kubernetes
k api-resources - returns all the k8 objects 

namesapce
-------------------

1.default               
2.kube-node-lease       
3.kube-public           
4.kube-system           
5.local-path-storage   

-----------------------

1.kubectl crerate ns  <namespace_name> --> to crreate namesapce

2.kubectl get po -n <namespace_name> - to access the resources under namespace. If -n & namepsace
is not specified , it will return from default



3.cluster components are created in kube system
master related components are stored in kube system

kubectl get po -n kube-system -> returns all the pod from kubesystem

kube proxy provides network for nodes
------------------------------------------------
In deployment

IN replicaset controller is created , under that POD is created 
ways to create Pod
1. imperative
2. yaml

-------------------
autoscaling

kubneretes supports only cpu utilization 

we create HPA (horizontal pod autoscaling )




kind create cluster --config config
  311  docker images
  312  kubectl get nodes
  313  docker ps
  317  kubectl api-resources
  318  alias k=kubectl
  319 
  320  k api-resources
  321  k get ns
  322  k get -n po
  323  k get  po
  324  k create ns myns
  325  k get nodes
  326  k get node
  327  clear
  328  k get ns
  329  k get -n myns
  330  k get po
  331  k get -n myns po
  332  k get po -n myns
  333  k get po 
  335  k get po -n kube-system
  337  k get po -n kube-system -o wide
  339  k run firstpod --image=kavana97/first:1.0 --port=8080
  340  k get pod
  341  k get po -o wide
  342  k get node
  343  k describe firstpod
  344  k describe po firstpod
  345  k logs firstpod
  347  k get po
  348  k exec -it firstpod bash
  349  kubectl exec -it firstpod bash
  350  kubectl exec -it firstpod -- bash
  351  k exec -it firstpod -- bash
  353  k delete po firstpod
  354  k get pod
  355 
  356  docker ps
  -----------------------
to create deployment

  k create deploy <mydeploy> --image=kavana97/first:1.0 --port=8080 --dry-run=client -o yaml > delpoy.yaml
  k apply -f mydeploy.yaml

  k get deploy mydeploy
  k get po
  k get rs  > rs > replica set
  -------------------------

  to create pod 

  k run <firstpod>  --image=kavana97/first:1.0 --port=8080 --dry-run=client -o yaml > pod.yaml
  k apply -f pod.yaml

  -----------------------------
  to edit replica set 

  3 ways

  1. vi deploy.yaml
  2. k scale deploy <mydeploy> --replicas=10
  3.l edit deploy <mydeploy> -- this will edit in live object

  autoscale --> horizontal pod autoscale -> hpa

k autoscale deploy <mydeploy> --min=2 --max=10 --cpu-percent=70

k rollout status deploy <mydeploy>
k rollout history deploy <mydeploy> --> how time particular version got updated

k set image deploy <downtime> nginx=nginx:1.8.1 --record  -- to update

k rollout undo deploy <mydeeploy> -> to previous

k rollout undo deploy <mydeploy> --to-revision=2

------------------------

on k8 , there is no guarantee application will node on particular node . sO IT is difficult to change the pod . We have to bind by service object

Service
--------
2. types
1. Node port -> to access outside the cluster
2. Cluster ip -> inside the cluster
---------------------------------

k run nginx --image=nginx -port=80

k expose po nginx --name <myservice> type=NodePort --port=80 --target-port=80  --to expose
------------------------------------------------------------------------------

Scheduling

Lets say application running on port has to have ssd memory , in that case app will run on pod having ssd memory in very low of chance.

k get no --show-labels -> to see the labels

k label no <node-name for example kind-worker> hdd=ssd -> assigning the label

k label no <kind-worker> hdd-   --> to remove the label

In Scheduling
1. Node selector --> schedule the pod if checks (labels) are matching
2. Node affinity -> hard & soft

In hard -> schedule the pod if checks (labels) are matching
in soft -> don't schedule the if key and value is available . else schedule it if any other pod is not available
-------------

Taint and tolerance

Taint -> on node
tolerance -> on pod

effect
1. Noschedule -> if taint is not matching tolerance, then don't schedule pod  on node,lets say taint is applying now, existing node will run
2. Preferschedule -> if taint is not matching tolerance for a pod try not to schedule the node. but if any other node is available then you schedule, existing will run
3. Noexecute -> if taint is not matching tolerance, then don't schedule pod  on node,lets say taint is applying now, existing node will get removed

none of the pod is created in master node. bcz by default master node has taint

k taint no kind-worker hdd=ssd: Noschedule  --> to create taint

<hdd=ssd: Noschedule > === <key=value:effect>

 k cordon kind-worker2 - it is used to disable the schedule

 ------------------------------
 Daemonset

 1.It will create pod in each node . 
 2.if node has taint , then it has to have the tolerance for pod , then only it eill get crreated on tainted node

 In deployment , there is no guarantee that pod will get created in each node

 
 ----------------------------

 Job and cronjob

 watch kubectl get all -> to watches
 /bin/sh -c "sleep 10" -> to pass command


 If we want to have the backup of an application, 

 in case of deploy , it will create the pod again if we stop also

 in case of job , once the job is completed , pod will be in completed stage . If the pod get stopped in between of any job , then pod will get created

 k create job <myjobname> --image=ubuntu:22.04 --dry-run=client -o yaml -- /bin/sh -c "sleep 10" >job.yaml -> to create job
 k apply -f job.yaml

 vi job.yaml ->
 under spec , completions is for multiple
 parallelism is for multiple job at same time

 to create job 

 kubectl create cj <cjname> --image=ubuntu:22.04 --schedule="*/1 * * * *" --dry-run=client -o yaml -- /bin/sh -c "sleep 10" > mycj.yaml
 kubectl appply -f mycj.yaml

 watch kubectl get all  
 

 -------------------------------------

 configMap

 kubectl create cm <mycm> --from-literal=<key db-host>=<value 192.168.0.0>

 kubectl create cm <configfile> --from-file=<filename.ini>


 ----------------------------------------------------------------------------------------------------

 Whenever cluster gets created , by default admin user get user . 

 all the details of admin user will be stored in home directory ~/.kube/config


~/.kube/config created when cluster is created

kind-kind is admin user

rollback authentication control - RBAC - where we put restriction ( this user can create a deployment object or not)

-------------------------------------------------------------------------------------------------------------------

Volume

2 type

1. Static Volume provisioning - first ,persistence Volume -> persistence Volume claim -> pod needs to created
2. Dynamic Volume provisioning - first,  pod -> persistence Volume -> persistence Volume claim 

for Dynamic Volume provisioning ,
k8 provides storage class , for that we need to provide an attribute called " provisioner" . 
by script

2 object 
1. persistence Volume
2. persistence Volume claim

Access mode
1. readWriteOnce
2. readWriteMany
3. readOnlyMany


Recliam policy - after delete what should happen for pod
2 types
1. Retain 
2. delete


-----------------------------------------

difference between deployment & statefulset is in word
----------------------------------
to scale the statefulset

kubectl scale sts <name> --replicas=10

------------------------------

network policy

default CNI cluster is kindnet . it doesn't support network policy . 

so config fille needts to change to


kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
networking:
  disableDefaultCNI: true
  podSubnet: 192.168.0.0/16


  then install 
 1. kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/tigera-operator.yaml

2.kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/custom-resources.yaml

then ,,, watch kubectl get pods -l k8s-app=calico-node -A


reference -> https://docs.tigera.io/calico/latest/getting-started/kubernetes/kind

-----------------------------


kubeconfig

kubectl create cluster --name <mycluster > -> this will create one container , that will act as master as well as worker

1 . kubectl config -h
 kubectl config <get-clusters or get-users >

 kubectl config use-context <cluester-name> -> to switch between cluster

 cluset name can be determined by , kubectl config get-clusters
 ------------------------------------------------------


 RBAC

 RBAC is a model designed to grant access to resources based on the roles of individual users within an organization.

 https://learnk8s.io/rbac-kubernetes

 4 objects
 1. role
 2. ClusterRole
 3. Rolebinding
 4. ClusterRoleBinding

 steps is in https://github.com/kavana94488/k8s_material

 -------------------------------------------


 Node maintainance

 1. kubectl drain <node-name > 
 2. kubectl uncordon kind-worker2 , to allow Scheduling
--------------------

service account & serviceContext

kubectl create sa <mysa>  -> to create service account

get inside the po , cd proc/1 , vi status

https://github.com/torvalds/linux/blob/master/include/uapi/linux/capability.h

--------------------


 1. helm ls - to list

 2. helm create <myapp>

  3.  helm  template <myapp> <myapp>/ -> this will give what is going to get created

  4.  helm install <myapp> <myapp>/  -> to deploy
 
 helm upgrade <myapp> <myapp>/ -> to update

 curl 172.18.0.3  : 30085
      <ip of no > : <port of service>

------------------

apt-cache madison kubeadm

-------------------------

security context reference

https://www.dynatrace.com/engineering/blog/kubernetes-security-part-3-security-context/



https://bluexp.netapp.com/blog/3-ways-to-save-big-and-10-price-variations-to-know-aws-cvo-blg

------------------------

to remove taint

 kubectl taint no <master , ie hostname> node-role.kubernetes.io/control-plane:NoSchedule-


node-role.kubernetes.io/control-plane:NoSchedule -> will get in taint <--- we hv to describe the node
--------------

prometheus is a time series database

we have to have scrape_interval like duration & target - where app is running